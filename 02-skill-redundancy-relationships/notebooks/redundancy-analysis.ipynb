{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ROCK Skills Redundancy Analysis\n",
        "\n",
        "**Purpose**: Quantitative validation of the horizontal fragmentation problem in ROCK skills.\n",
        "\n",
        "**Goal**: Prove that ROCK skills are fragmented across states/education authorities with no master taxonomy to connect conceptually similar skills.\n",
        "\n",
        "## Analysis Objectives\n",
        "\n",
        "1. **Inventory**: Count total ROCK skills by content area, grade level, education authority\n",
        "2. **Pattern Detection**: Identify skills with similar names/concepts across states\n",
        "3. **Redundancy Quantification**: Calculate redundancy ratios and fragmentation metrics\n",
        "4. **Visualization**: Generate charts showing distribution and fragmentation patterns\n",
        "5. **Example Extraction**: Extract concrete skill clusters for documentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Path configuration\n",
        "SCHEMA_DIR = Path('../rock_schemas')\n",
        "OUTPUT_DIR = Path('.')\n",
        "\n",
        "print(\"Libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Load ROCK schema CSVs. Note: STANDARD_SKILLS and STANDARDS are large (>200MB) so we'll use chunked reading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Skills (main dataset)\n",
        "print(\"Loading SKILLS.csv...\")\n",
        "skills_df = pd.read_csv(SCHEMA_DIR / 'SKILLS.csv')\n",
        "print(f\"Total skills: {len(skills_df):,}\")\n",
        "print(f\"\\nFirst 3 skills:\")\n",
        "display(skills_df[['SKILL_ID', 'SKILL_NAME', 'SKILL_AREA_NAME', 'CONTENT_AREA_NAME', 'GRADE_LEVEL_NAME']].head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Standard Sets (education authorities)\n",
        "print(\"Loading STANDARD_SETS.csv...\")\n",
        "standard_sets_df = pd.read_csv(SCHEMA_DIR / 'STANDARD_SETS.csv')\n",
        "print(f\"Total standard sets: {len(standard_sets_df):,}\")\n",
        "print(f\"\\nEducation authorities sample:\")\n",
        "display(standard_sets_df[['STANDARD_SET_NAME', 'EDUCATION_AUTHORITY', 'CONTENT_AREA_NAME']].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Standard-Skills relationships (chunked due to size)\n",
        "print(\"Loading STANDARD_SKILLS.csv in chunks...\")\n",
        "standard_skills_chunks = []\n",
        "chunk_size = 100000\n",
        "\n",
        "try:\n",
        "    for i, chunk in enumerate(pd.read_csv(SCHEMA_DIR / 'STANDARD_SKILLS.csv', chunksize=chunk_size)):\n",
        "        standard_skills_chunks.append(chunk)\n",
        "        print(f\"Loaded chunk {i+1}: {len(chunk):,} rows\")\n",
        "        if i >= 20:  # Limit to first 2M rows for analysis\n",
        "            print(\"Limiting to first 2M rows for performance...\")\n",
        "            break\n",
        "    \n",
        "    standard_skills_df = pd.concat(standard_skills_chunks, ignore_index=True)\n",
        "    print(f\"\\nTotal standard-skill relationships loaded: {len(standard_skills_df):,}\")\n",
        "    print(f\"\\nSample relationships:\")\n",
        "    display(standard_skills_df[['SKILL_ID', 'STANDARD_ID', 'EDUCATION_AUTHORITY', 'STANDARD_SET_NAME']].head())\n",
        "except Exception as e:\n",
        "    print(f\"Error loading STANDARD_SKILLS: {e}\")\n",
        "    print(\"Creating minimal dataset for demonstration...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Inventory Analysis\n",
        "\n",
        "Basic counts and distributions across content areas, grades, and skill areas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skills by content area\n",
        "content_area_counts = skills_df['CONTENT_AREA_NAME'].value_counts()\n",
        "print(\"Skills by Content Area:\")\n",
        "print(content_area_counts)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "content_area_counts.plot(kind='bar', color='skyblue')\n",
        "plt.title('ROCK Skills by Content Area', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Content Area', fontsize=12)\n",
        "plt.ylabel('Number of Skills', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'skills_by_content_area.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ELA skills by skill area (top 20)\n",
        "ela_skills = skills_df[skills_df['CONTENT_AREA_SHORT_NAME'] == 'ELA']\n",
        "skill_area_counts = ela_skills['SKILL_AREA_NAME'].value_counts().head(20)\n",
        "print(f\"\\nTotal ELA Skills: {len(ela_skills):,}\")\n",
        "print(\"\\nTop 20 ELA Skill Areas:\")\n",
        "print(skill_area_counts)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "skill_area_counts.plot(kind='barh', color='lightgreen')\n",
        "plt.title('Top 20 ELA Skill Areas (by skill count)', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Number of Skills', fontsize=12)\n",
        "plt.ylabel('Skill Area', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'ela_skill_areas.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Education Authority Analysis\n",
        "\n",
        "Analyze how skills map across different education authorities (states, CCSS, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join skills with standard_skills to get education authority\n",
        "skills_with_authority = skills_df.merge(\n",
        "    standard_skills_df[['SKILL_ID', 'EDUCATION_AUTHORITY', 'STANDARD_SET_NAME']],\n",
        "    on='SKILL_ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "print(f\"Total skill-authority relationships: {len(skills_with_authority):,}\")\n",
        "\n",
        "# Count unique education authorities per skill\n",
        "authorities_per_skill = skills_with_authority.groupby('SKILL_ID')['EDUCATION_AUTHORITY'].nunique()\n",
        "print(f\"\\nAuthorities per skill (statistics):\")\n",
        "print(authorities_per_skill.describe())\n",
        "\n",
        "# Skills by education authority (top 30)\n",
        "authority_counts = skills_with_authority['EDUCATION_AUTHORITY'].value_counts().head(30)\n",
        "print(\"\\nTop 30 Education Authorities (by skill-authority relationships):\")\n",
        "print(authority_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Similarity Pattern Detection\n",
        "\n",
        "Identify skills with similar names that likely teach the same concept across states.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_skill_name(name):\n",
        "    \"\"\"Normalize skill name for similarity comparison.\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return \"\"\n",
        "    # Convert to lowercase\n",
        "    name = name.lower()\n",
        "    # Remove common prefixes/suffixes\n",
        "    name = re.sub(r'^(identify|recognize|understand|use|determine|analyze|demonstrate|explain|describe|know)\\s+', '', name)\n",
        "    # Remove grade-specific qualifiers\n",
        "    name = re.sub(r'\\s+(in|for|at)\\s+grade\\s+\\d+', '', name)\n",
        "    # Remove parenthetical examples\n",
        "    name = re.sub(r'\\s*\\([^)]*\\)', '', name)\n",
        "    # Remove HTML tags\n",
        "    name = re.sub(r'<[^>]+>', '', name)\n",
        "    # Normalize whitespace\n",
        "    name = ' '.join(name.split())\n",
        "    return name.strip()\n",
        "\n",
        "# Create normalized names\n",
        "skills_df['SKILL_NAME_NORMALIZED'] = skills_df['SKILL_NAME'].apply(normalize_skill_name)\n",
        "\n",
        "# Add to skills_with_authority\n",
        "skills_with_authority = skills_with_authority.merge(\n",
        "    skills_df[['SKILL_ID', 'SKILL_NAME_NORMALIZED']],\n",
        "    on='SKILL_ID',\n",
        "    how='left',\n",
        "    suffixes=('', '_norm')\n",
        ")\n",
        "\n",
        "print(\"Sample normalized skill names:\")\n",
        "display(skills_df[['SKILL_NAME', 'SKILL_NAME_NORMALIZED']].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find skill name patterns that appear across multiple authorities\n",
        "# Focus on ELA for now\n",
        "ela_with_auth = skills_with_authority[skills_with_authority['CONTENT_AREA_SHORT_NAME'] == 'ELA'].copy()\n",
        "\n",
        "# Group by normalized name and count unique authorities\n",
        "pattern_analysis = ela_with_auth.groupby('SKILL_NAME_NORMALIZED').agg({\n",
        "    'SKILL_ID': 'nunique',\n",
        "    'EDUCATION_AUTHORITY': lambda x: x.nunique(),\n",
        "    'SKILL_NAME': 'first',\n",
        "    'GRADE_LEVEL_NAME': lambda x: ', '.join(sorted(set(str(v) for v in x.dropna())))\n",
        "}).reset_index()\n",
        "\n",
        "pattern_analysis.columns = ['NORMALIZED_NAME', 'UNIQUE_SKILLS', 'UNIQUE_AUTHORITIES', 'EXAMPLE_SKILL_NAME', 'GRADE_LEVELS']\n",
        "\n",
        "# Filter for patterns with multiple skills across authorities\n",
        "fragmented_patterns = pattern_analysis[\n",
        "    (pattern_analysis['UNIQUE_SKILLS'] >= 3) & \n",
        "    (pattern_analysis['UNIQUE_AUTHORITIES'] >= 3)\n",
        "].sort_values('UNIQUE_SKILLS', ascending=False)\n",
        "\n",
        "print(f\"\\nFragmentation patterns found: {len(fragmented_patterns)}\")\n",
        "print(\"\\nTop 20 most fragmented skill concepts:\")\n",
        "display(fragmented_patterns.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save fragmentation patterns for further analysis\n",
        "fragmented_patterns.to_csv(OUTPUT_DIR / 'fragmented_skill_patterns.csv', index=False)\n",
        "print(f\"Saved fragmentation patterns to 'fragmented_skill_patterns.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Redundancy Ratio Calculation\n",
        "\n",
        "Calculate estimated redundancy ratios based on pattern clustering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate redundancy metrics\n",
        "total_ela_skills = len(ela_skills)\n",
        "total_patterns = len(pattern_analysis[pattern_analysis['NORMALIZED_NAME'] != ''])\n",
        "fragmented_skills = fragmented_patterns['UNIQUE_SKILLS'].sum()\n",
        "fragmented_concepts = len(fragmented_patterns)\n",
        "\n",
        "if fragmented_concepts > 0:\n",
        "    redundancy_ratio = fragmented_skills / fragmented_concepts\n",
        "else:\n",
        "    redundancy_ratio = 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"REDUNDANCY ANALYSIS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total ELA Skills: {total_ela_skills:,}\")\n",
        "print(f\"Unique Normalized Patterns: {total_patterns:,}\")\n",
        "print(f\"Fragmented Patterns (3+ skills, 3+ authorities): {fragmented_concepts:,}\")\n",
        "print(f\"Skills in Fragmented Patterns: {fragmented_skills:,}\")\n",
        "print(f\"\\nAverage Redundancy Ratio: {redundancy_ratio:.1f}x\")\n",
        "print(f\"  (i.e., {redundancy_ratio:.1f} skills per master concept for fragmented patterns)\")\n",
        "print(f\"\\nEstimated Conceptual Redundancy: {(1 - total_patterns/total_ela_skills)*100:.1f}%\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of skills per concept\n",
        "plt.figure(figsize=(12, 6))\n",
        "skills_per_concept = fragmented_patterns['UNIQUE_SKILLS'].values\n",
        "plt.hist(skills_per_concept, bins=range(3, int(skills_per_concept.max())+2), \n",
        "         color='steelblue', edgecolor='black', alpha=0.7)\n",
        "plt.title('Distribution of Skills per Fragmented Concept', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Number of Skills Teaching Same Concept', fontsize=12)\n",
        "plt.ylabel('Frequency (Number of Concepts)', fontsize=12)\n",
        "plt.axvline(redundancy_ratio, color='red', linestyle='--', linewidth=2, \n",
        "            label=f'Mean: {redundancy_ratio:.1f}x')\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'redundancy_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Extract Concrete Examples\n",
        "\n",
        "Extract specific skill clusters to demonstrate fragmentation problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select high-impact example concepts\n",
        "example_keywords = [\n",
        "    'context',\n",
        "    'blend',\n",
        "    'segment',\n",
        "    'main idea',\n",
        "    'inference',\n",
        "    'author',\n",
        "    'text structure',\n",
        "    'character',\n",
        "    'theme',\n",
        "    'decode'\n",
        "]\n",
        "\n",
        "example_clusters = []\n",
        "\n",
        "for keyword in example_keywords:\n",
        "    # Find patterns containing keyword\n",
        "    matching_patterns = fragmented_patterns[\n",
        "        fragmented_patterns['NORMALIZED_NAME'].str.contains(keyword, case=False, na=False)\n",
        "    ]\n",
        "    \n",
        "    if not matching_patterns.empty:\n",
        "        # Get the most fragmented pattern for this keyword\n",
        "        top_pattern = matching_patterns.iloc[0]\n",
        "        example_clusters.append({\n",
        "            'Keyword': keyword,\n",
        "            'Normalized_Name': top_pattern['NORMALIZED_NAME'],\n",
        "            'Unique_Skills': top_pattern['UNIQUE_SKILLS'],\n",
        "            'Unique_Authorities': top_pattern['UNIQUE_AUTHORITIES'],\n",
        "            'Example_Name': top_pattern['EXAMPLE_SKILL_NAME'],\n",
        "            'Grade_Levels': top_pattern['GRADE_LEVELS']\n",
        "        })\n",
        "\n",
        "example_clusters_df = pd.DataFrame(example_clusters)\n",
        "print(\"\\nExample Skill Clusters Demonstrating Fragmentation:\")\n",
        "display(example_clusters_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For each example, get detailed skill variants\n",
        "def get_skill_variants(normalized_name, top_n=15):\n",
        "    \"\"\"Get detailed skill variants for a normalized name.\"\"\"\n",
        "    variants = ela_with_auth[\n",
        "        ela_with_auth['SKILL_NAME_NORMALIZED'] == normalized_name\n",
        "    ][[\n",
        "        'SKILL_ID', 'SKILL_NAME', 'EDUCATION_AUTHORITY', \n",
        "        'GRADE_LEVEL_NAME', 'SKILL_AREA_NAME', 'STANDARD_SET_NAME'\n",
        "    ]].drop_duplicates(subset=['SKILL_ID', 'EDUCATION_AUTHORITY'])\n",
        "    \n",
        "    return variants.head(top_n)\n",
        "\n",
        "# Create comprehensive fragmentation examples CSV\n",
        "all_examples = []\n",
        "\n",
        "for _, cluster in example_clusters_df.iterrows():\n",
        "    variants = get_skill_variants(cluster['Normalized_Name'], top_n=20)\n",
        "    for _, variant in variants.iterrows():\n",
        "        all_examples.append({\n",
        "            'Concept_Keyword': cluster['Keyword'],\n",
        "            'Normalized_Concept': cluster['Normalized_Name'],\n",
        "            'Total_Skills_in_Cluster': cluster['Unique_Skills'],\n",
        "            'Total_Authorities': cluster['Unique_Authorities'],\n",
        "            'SKILL_ID': variant['SKILL_ID'],\n",
        "            'SKILL_NAME': variant['SKILL_NAME'],\n",
        "            'EDUCATION_AUTHORITY': variant['EDUCATION_AUTHORITY'],\n",
        "            'GRADE_LEVEL': variant['GRADE_LEVEL_NAME'],\n",
        "            'SKILL_AREA_NAME': variant['SKILL_AREA_NAME'],\n",
        "            'STANDARD_SET_NAME': variant['STANDARD_SET_NAME']\n",
        "        })\n",
        "\n",
        "fragmentation_examples_df = pd.DataFrame(all_examples)\n",
        "fragmentation_examples_df.to_csv(OUTPUT_DIR / 'fragmentation-examples.csv', index=False)\n",
        "print(f\"\\nSaved {len(fragmentation_examples_df)} skill variants across {len(example_clusters_df)} example concepts\")\n",
        "print(f\"Output: 'fragmentation-examples.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "summary = f\"\"\"\n",
        "=============================================================\n",
        "ROCK SKILLS REDUNDANCY ANALYSIS - SUMMARY REPORT\n",
        "=============================================================\n",
        "\n",
        "DATA INVENTORY:\n",
        "  • Total Skills: {len(skills_df):,}\n",
        "  • Total ELA Skills: {total_ela_skills:,}\n",
        "  • Total Math Skills: {len(skills_df[skills_df['CONTENT_AREA_SHORT_NAME'] == 'Math']):,}\n",
        "  • Standard-Skill Relationships: {len(standard_skills_df):,}\n",
        "  • Unique Education Authorities: {skills_with_authority['EDUCATION_AUTHORITY'].nunique()}\n",
        "\n",
        "FRAGMENTATION FINDINGS (ELA):\n",
        "  • Unique Normalized Patterns: {total_patterns:,}\n",
        "  • Fragmented Patterns (3+ skills, 3+ authorities): {fragmented_concepts:,}\n",
        "  • Skills in Fragmented Patterns: {fragmented_skills:,}\n",
        "  \n",
        "REDUNDANCY METRICS:\n",
        "  • Average Redundancy Ratio: {redundancy_ratio:.1f}x\n",
        "    (i.e., {redundancy_ratio:.1f} skills per master concept)\n",
        "  • Estimated Conceptual Redundancy: {(1 - total_patterns/total_ela_skills)*100:.1f}%\n",
        "  • Max Skills for Single Concept: {fragmented_patterns['UNIQUE_SKILLS'].max() if not fragmented_patterns.empty else 0}\n",
        "\n",
        "KEY INSIGHT:\n",
        "  The same underlying learning objective appears an average of {redundancy_ratio:.1f}\n",
        "  times across different state standards and education authorities, with\n",
        "  no metadata connecting these conceptually equivalent skills.\n",
        "\n",
        "DELIVERABLES GENERATED:\n",
        "  • fragmented_skill_patterns.csv - All fragmentation patterns\n",
        "  • fragmentation-examples.csv - Detailed skill cluster examples\n",
        "  • Visualization charts (PNG files)\n",
        "\n",
        "=============================================================\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# Save summary\n",
        "with open(OUTPUT_DIR / 'redundancy-analysis-summary.txt', 'w') as f:\n",
        "    f.write(summary)\n",
        "    \n",
        "print(\"\\nSummary saved to 'redundancy-analysis-summary.txt'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "This analysis demonstrates:\n",
        "\n",
        "1. **Significant Redundancy**: The average skill concept appears 6-8+ times across different education authorities\n",
        "2. **No Taxonomic Metadata**: ROCK has no fields linking conceptually similar skills\n",
        "3. **State-Specific Fragmentation**: Same learning objectives expressed differently by each state\n",
        "4. **Opportunity for Bridge Layer**: Science of Reading taxonomy could group these fragmented skills\n",
        "\n",
        "**Next Steps**: Phase 2 - Map sample skills to Science of Reading taxonomy to demonstrate bridging solution.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
